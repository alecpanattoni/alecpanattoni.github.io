---
layout: default
title: Missingness' Effect on Fairness
---
<head>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@500&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/main. css" />
</head>

---
layout: default
title: Missingness' Effect on Fairness
---
<head>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@500&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/main. css" />
</head>

<div class="blurb">
	<h1>Evaluating the Effect of Data Missingness on Fairness of Machine Learning Algorithms</h1>
	<p class="text">Can you guess what my favorite kind of lunch meat is? <a href="https://youtu.be/dQw4w9WgXcQ">Yes, it is baloney</a></p>

	<p class="text">Welcome to the website for our project! Here we will be introducing the core ideas of our research project, the motivation behind it, and what work we have done in order to answer the question:
		Does Data Missingness affect Fairness in Machine Learning?
	</p>

	<h2>
		Introduction
	</h2>
	<h3>
		What's Fairness?
	</h3>
	<p class="text">
		Fairness is a degree to which a machine learning algorithm is able to adequately respond to bias present in data. Pretty much all data in the world is biased in some way due to a variety of
		factors: it may be an issue of data collection, it may be a deliberate decision to bias the data by favoring a certain population, or it might be caused by a confounding factor that is not accounted for.
		What we care about in our project is something known as Selection Bias: when the data that is present tells a story that is different from the real world situation.
	</p>

	<h3>
		How Does Missingness Play Into It?
	</h3>
	<p class="text">
		Missingness is a common occurence in the world of data science, and the term itself refers to a pattern (or lack thereof) of missing observations in the dataset. An example would be 
		a dataset that contains information on various vehicles registered at a local DMV, and the observations that are missing are some license plate numbers. The fact that there are multiple
		data points missing and that there might be an underlying pattern constitutes the concept of missingness.<br>

		If some data is missing, then it means that the data might not be representative of the real world; however, the way the data is missing determines how different the story the data tells
		is from the actual population. Data missing because of data entry errors is different from data being missing because those who recorded the data simply did not record certain values. So,
		data missingness is generally categorized based on the patterns of missingness and on dependencies between different variables (i.e. what the observations are recorded for, the typical "columns"
		in a data table). <br>

		The following types are generally identified and these are the ones we focus on in our research:
		<ul class="text">
			<li>
				Data Missing Completely at Random (MCAR). This type of missingness occurs when there is no pattern to why the data is missing; it is as if the observations are taken out by pure chance.
				This type of missingness means that we can't establish how and why the data is made missing, so the way it can affect fairness is unpredictable.
			</li>
			<li>
				Data Missing at Random (MAR). This type of missingness occurs when the chances of observation missing is based on other variables, but not the variable itself. Following a previous example,
				it's if we determine that the license plate numbers are missing more often for certain car makes than others. This type of missigness is more helpful to the topic of fairness, as it can help
				us identify how certain groups in the dataset might be represented due to a pattern of missing observations, misrepresenting the actual values for the population.
			</li>
			<li>	
				Data Not Missing at Random (NMAR). This type of missingness occurs when the value of the observation itself determines its chances to be missing. In the DMV example, this would happen if all the license
				plates starting with letter "A" were missing. This type of missingness is also important for fairness, as it might indicate that certain values are not accounted for due to the value itself being 
				in a certain range, thus hinting at a possible misrepresentation of the real world situation.
			</li>
			<li>
				Data Missing by Design (MD). This type of missingness is not covered by us, but is usually identified when missigness is mentioned. This simply indicates that the data is expected to be missing, and
				this usually occurs when there are variables for which the observation just might not occur naturally. A typical example is a survey: there are several columns based on the answers the person gave,
				but also another which states whether the person agreed to the survey or not; if the latter states "No", then all other columns will have missing data because the person did not do the survey.
			</li>
		</ul>
	</p class="text">

	<h2>
		Our Approach to the Question
	</h2>

	<h3>
		Data
	</h3>
	<p class="text">
		In order to work on our research question, we needed a dataset which could be biased, and also a dataset that contains the specific missingness patterns we are trying to evaluate. As a result,
		we decided to create a synthetic dataset based on 
		<a href="https://www.kaggle.com/datasets/asimislam/civilian-complaints-against-nyc-police-officers?select=allegations_20200726939.csv">
			real data collected on citizens' complaints to the New York Polic Department
		</a>. We used the data because it contains information about the complainants' gender, ethnicity, age, and other attributes that could be discriminated upon by the police department. Therefore, the
		data might show some biases, and the algorithm that processes this data might pick up on those biases and produce unfair decisions. <br>
		
		Our dataset is syntetic because we take the original dataset and 
		simulate our own distribution of missingness. The process by which this works is that we use statistical representations of types of missingness in order to select which observations to "drop" from the dataset.
		As a result, we end up with a dataset that has certain values missing, and those are determined by a pattern we decide. If you want to know more about how we simulate our dataset, feel free to visit
		<a href="/methods">Methods In-Depth</a> page.
	</p>

	<h3>
		Processing and Evaluating Methods
	</h3>

	<h3>
		Results and Interpretation
	</h3>

	<h3>
		What We Learned
	</h3>
</div><!-- /.blurb -->
