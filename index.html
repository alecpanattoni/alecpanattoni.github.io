---
layout: default
title: Missingness' Effect on Fairness
---
<head>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@500&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="css/main. css" />
</head>

<div class="blurb">
	<h1>Evaluating the Effect of Data Missingness on Fairness of Machine Learning Algorithms</h1>

	<p class="text">Welcome to the website for our project! Here we will be introducing the core ideas of our research project, the motivation behind it, and what work we have done in order to answer the question:
		Does Data Missingness affect Fairness in Machine Learning?
	</p>

	<h2>
		Introduction
	</h2>
	<h3>
		What's Fairness?
	</h3>
	<p class="text">
		Fairness is a degree to which a machine learning algorithm is able to adequately respond to bias present in data. Pretty much all data in the world is biased in some way due to a variety of
		factors: it may be an issue of data collection, it may be a deliberate decision to bias the data by favoring a certain population, or it might be caused by a confounding factor that is not accounted for.
		What we care about in our project is something known as Selection Bias: when the data that is present tells a story that is different from the real world situation.
	</p>

	<h3>
		How Does Missingness Play Into It?
	</h3>
	<p class="text">
		Missingness is a common occurence in the world of data science, and the term itself refers to a pattern (or lack thereof) of missing observations in the dataset. An example would be 
		a dataset that contains information on various vehicles registered at a local DMV, and the observations that are missing are some license plate numbers. The fact that there are multiple
		data points missing and that there might be an underlying pattern constitutes the concept of missingness.<br>

		If some data is missing, then it means that the data might not be representative of the real world; however, the way the data is missing determines how different the story the data tells
		is from the actual population. Data missing because of data entry errors is different from data being missing because those who recorded the data simply did not record certain values. So,
		data missingness is generally categorized based on the patterns of missingness and on dependencies between different variables (i.e. what the observations are recorded for, the typical "columns"
		in a data table). <br>

		The following types are generally identified and these are the ones we focus on in our research:
		<ul class="text">
			<li>
				Data Missing Completely at Random (MCAR). This type of missingness occurs when there is no pattern to why the data is missing; it is as if the observations are taken out by pure chance.
				This type of missingness means that we can't establish how and why the data is made missing, so the way it can affect fairness is unpredictable.
			</li>
			<li>
				Data Missing at Random (MAR). This type of missingness occurs when the chances of observation missing is based on other variables, but not the variable itself. Following a previous example,
				it's if we determine that the license plate numbers are missing more often for certain car makes than others. This type of missigness is more helpful to the topic of fairness, as it can help
				us identify how certain groups in the dataset might be represented due to a pattern of missing observations, misrepresenting the actual values for the population.
			</li>
			<li>	
				Data Not Missing at Random (NMAR). This type of missingness occurs when the value of the observation itself determines its chances to be missing. In the DMV example, this would happen if all the license
				plates starting with letter "A" were missing. This type of missingness is also important for fairness, as it might indicate that certain values are not accounted for due to the value itself being 
				in a certain range, thus hinting at a possible misrepresentation of the real world situation.
			</li>
			<li>
				Data Missing by Design (MD). This type of missingness is not covered by us, but is usually identified when missigness is mentioned. This simply indicates that the data is expected to be missing, and
				this usually occurs when there are variables for which the observation just might not occur naturally. A typical example is a survey: there are several columns based on the answers the person gave,
				but also another which states whether the person agreed to the survey or not; if the latter states "No", then all other columns will have missing data because the person did not do the survey.
			</li>
		</ul>
	</p class="text">

	<h2>
		Our Approach to the Question
	</h2>

	<h3>
		Data
	</h3>
	<p class="text">
		In order to work on our research question, we needed a dataset which could be biased, and also a dataset that contains the specific missingness patterns we are trying to evaluate. As a result,
		we decided to create a synthetic dataset based on 
		<a href="https://www.kaggle.com/datasets/asimislam/civilian-complaints-against-nyc-police-officers?select=allegations_20200726939.csv">
			real data collected on citizens' complaints to the New York Police Department
		</a>. We used the data because it contains information about the complainants' gender, ethnicity, age, and other attributes that could be discriminated upon by the police department. Therefore, the
		data might show some biases, and the algorithm that processes this data might pick up on those biases and produce unfair decisions. <br>
		
		Our dataset is synthetic because we take the original dataset and 
		simulate our own distribution of missingness. The process by which this works is that we use statistical representations of types of missingness in order to select which observations to "drop" from the dataset.
		As a result, we end up with a dataset that has certain values missing, and those are determined by a pattern we decide. If you want to know more about how we simulate our dataset, feel free to visit
		<a href="/methods">Methods In-Depth</a> page.
	</p>

	<h3>
		Processing and Evaluating Methods
	</h3>

	<p class="text">
		After generating our synthetic dataset, we apply the dataset repair in order to approach the biases in the dataset that result from introduced missingness. In order to evaluate bias,
		we use what is known as fairness notions: mathematical ways of quantifying whether a single group is more preferred than another by the algorithm. The notions that we use are:
		statistical parity, equality of odds (EOdd), and equality of opportunity (EOpp); we cover the meaning of each fairness notion more in the <a href="/methods.html">relevant methods supbage</a>.<br>

		The dataset repair is essentially reconfiguring the dataset in a way that removes the biased dependencies of values; the algorithm that we use is an in-processing algorithm of Adversarial Debiasing, which we import from the AIF360 package.
		This package is commonly used for approaches like ours, where we need to make changes to the dataset throughout the application of the algorithm in order to address the biases in the data.<br>

		Adversarial Debiasing is applied to the three datasets with generated missingness (that including MCAR, MAR, and NMAR patterns), as well as the dataset with no missingness whatsoever (i.e. all missing tuples are removed).
		After Adversarial Debiasing is applied for each dataset, we record the fairness notions in order to see how each type of missingness performed after in-processing.

	</p>

	<h3>
		Results and Interpretation
	</h3>

	<p class="text">
		As we evaluate our results, we first notice that our dataset with no initial missingness performs quite well in terms of fairness and accuracy.
		This serves as a baseline for what will most likely be the best performance for our model. This is because once we introduce missingness, there is
		now a chance that our model becomes more unfair and potentially inaccurate. It is also possible for the results to be more fair/accurate for any of
		these missingness types, but very unlikely. 
		<br>
		As we begin to introduce missingness such as NMAR, MAR and MCAR, we begin to see a difference in the measurements of fairness. To begin with, all of our 
		fairness measurements increase slightly, meaning that the missing data is finally having a effect on measuring the fairness of the dataset. While NMAR 
		seems to not really have a significant different fairness measurement from the dataset with no missing data, the fairness values increase more with MCAR. 
		Last but not least, MAR seems to perform the poorest out of all the types of missingness. While the accuracy for MAR deacreases, all of the statistical parity,
		equality of odds and equality of opportunity increases, indicating a poor performance in fairness. From what we have learned, we might see why the fairness measures 
		decrease for MAR. Because Missing at Random depends on other columns than the column with the missing values itself, unless we know the reasoning behind why the missing values 
		were chosen, it might be hard to determine why the values are missing. Therefore, it will perform poorly on determining fairness. 
 
		
		
	</p>

	<h3>
		What We Learned
	</h3>

	<p class="text">
		Throughout our project, we studied the effects of missingness on fairness, as well as how fairness itself can be evaluated and approached during the implimantion of the algorithm. Fairness
		can be evaluated using a series of fairness notions, and we learned not only what they are and how to calculate them, but also how to use them in order to work towards a debiased dataset.
		Fairness notions indicate whether a certain group is favored over another in the dataset; if we are able to reconstruct our dataset in a way that brings the notion closer to zero, that means
		that there is less difference between the groups. We learned how to use dataset repair techniques from packages such as AIF360 to approach this problem and to reshape the dataset in a way
		that creates a fairer picture of the real world while still keeping data representative of the population. 
		Throughout our project, we studied the effects of missingness on fairness.
		<br>
		A lot of what we learned lies in the definition of missingness, its distribution, and how we can statistically explain and evaluate missingness in the dataset. At the end of the day,
		missingness is just a statistical distribution of missing values in the dataset; as with any other recognizable pattern in the dataset, it can affect the outcome of the analysis. In our
		research, we learned about different types of bias and how it is mathetmatically defined; the relationships between sensitive and non-sensitive attributes and the final outcome, as well as
		the original labels used for training. All these together are linked in a causal graph (Salimi et al., 2019) that showcases the relationships between the attributes. If a sensitive attribute that
		is proven to be involved in causal relationships with the outcome has noticeable patterns of missing data, then it can severely skew the labels that the algorithm will be predicting.
		<br>
		Logically,
		missingness can be treated like any other data, but the difference in this case is that there is no story that can be told from this "data"; non-missing values are tied to specific outcomes
		in the real world, while the absence of values does not tell us or the algorithm anything about what is happening. Hence, we needed to see how absent data can affect the algorithm; how will the algorithm
		learn to predict the outcome if it is missing whole chunks of data that could be crucial to providing accurate and non-biased representation of the target population.
		

	</p>
</div><!-- /.blurb -->
